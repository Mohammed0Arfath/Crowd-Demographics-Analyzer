{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cm_kVxvSgh8B",
    "outputId": "e2fe7719-0811-429f-e5b5-7a70915679b7"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers pillow scikit-learn pandas tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xk21FkZgmAY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b25DagVRg5G5",
    "outputId": "039ef317-ad2a-4f6a-81e2-c28b75da7d51"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtrnIKieg7HZ",
    "outputId": "4e0d9a42-ab83-4aed-c749-88ae66d20fd7"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Re-download the dataset\n",
    "dataset_path = kagglehub.dataset_download('jangedoo/utkface-new')\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_Ad_8Sug7vW",
    "outputId": "2a4187a8-6cc9-43d9-a983-01857e4711f2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path where the dataset is downloaded\n",
    "dataset_path = \"/kaggle/input/utkface-new/UTKFace\"\n",
    "\n",
    "# List the files in the directory\n",
    "print(os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9QSpIcDhAl6"
   },
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age <= 10:\n",
    "        return \"0-10\"\n",
    "    elif age <= 20:\n",
    "        return \"11-20\"\n",
    "    elif age <= 30:\n",
    "        return \"21-30\"\n",
    "    elif age <= 40:\n",
    "        return \"31-40\"\n",
    "    elif age <= 50:\n",
    "        return \"41-50\"\n",
    "    else:\n",
    "        return \"50+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8464c8c916d44852a80b743681f8429e",
      "feb7f1bc245b48e2a64520c608569ea6",
      "a86e6ec52ed7470ba678662119e7c888",
      "a40f1731fcea47959a7773a2d1373ea7",
      "ccbf1d974dfd43469182c31b7df5af6f",
      "0f906018b14b4bb3a35750abbf336be6",
      "fe04e0727e85455c8f845c63a99625fd",
      "1fc860b218294d439b6211b5365bc1bf",
      "6bf6c9cfc3914f13b8a4d453c2e3e889",
      "c940e264f7814943991da5f65eefe154",
      "d4af5433a6b644a39b4879dac9913b94"
     ]
    },
    "id": "nHeSbboKhEBg",
    "outputId": "65ef5c05-bbd3-44bc-9e64-866235b1b2a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract file paths and labels\n",
    "image_paths, age_labels, gender_labels = [], [], []\n",
    "\n",
    "for filename in tqdm(os.listdir(dataset_path)):\n",
    "    image_path = os.path.join(dataset_path, filename)\n",
    "    temp = filename.split('_')\n",
    "    age = categorize_age(int(temp[0]))\n",
    "    gender = int(temp[1])  # 0: Male, 1: Female\n",
    "    image_paths.append(image_path)\n",
    "    age_labels.append(age)\n",
    "    gender_labels.append(gender)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'image': image_paths, 'age': age_labels, 'gender': gender_labels})\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "\n",
    "# Gender dictionary\n",
    "gender_dic = {0: 'Male', 1: 'Female'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0AjVXfwhGIA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def detect_face_yunet(image_path, target_size=(384, 384)):\n",
    "    \"\"\"Detects a face using YuNet and preprocesses it safely.\"\"\"\n",
    "\n",
    "    # Ensure the file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"⚠️ File not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"⚠️ Could not load image: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Ensure RGB format\n",
    "\n",
    "    # Initialize YuNet\n",
    "    yunet = cv2.FaceDetectorYN.create(\n",
    "        model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "        config='',\n",
    "        input_size=(320, 320),\n",
    "        score_threshold=0.6,  # Lower threshold to detect more faces\n",
    "        nms_threshold=0.3,\n",
    "        top_k=5000\n",
    "    )\n",
    "\n",
    "    yunet.setInputSize((img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Detect faces\n",
    "    _, faces = yunet.detect(img)\n",
    "\n",
    "    if faces is None or len(faces) == 0:\n",
    "        print(f\"⚠️ No face detected in: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Ensure YuNet returns at least (x, y, w, h)\n",
    "    face = faces[0][:4] if faces.shape[1] >= 4 else None\n",
    "\n",
    "    if face is None:\n",
    "        print(f\"⚠️ Face detection failed for: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    x, y, w, h = map(int, face)  # Convert to int for cropping\n",
    "\n",
    "    # ✅ **Clamp negative values to 0** ✅\n",
    "    x = max(0, x)\n",
    "    y = max(0, y)\n",
    "\n",
    "    # Ensure valid crop\n",
    "    if w <= 0 or h <= 0 or x >= img.shape[1] or y >= img.shape[0]:\n",
    "        print(f\"⚠️ Invalid face coordinates: {x}, {y}, {w}, {h}\")\n",
    "        return None\n",
    "\n",
    "    cropped_face = img[y:y + h, x:x + w]\n",
    "\n",
    "    # Resize safely\n",
    "    if cropped_face.size == 0:\n",
    "        print(f\"⚠️ Empty cropped face for: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    cropped_face = cv2.resize(cropped_face, target_size)\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size=(384, 384)):\n",
    "    \"\"\"\n",
    "    Preprocess the image with resizing, enhancement, noise reduction, etc.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Resize and convert to RGB\n",
    "    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    img = img.convert(\"RGB\")\n",
    "\n",
    "    # Enhance contrast\n",
    "    contrast_enhancer = ImageEnhance.Contrast(img)\n",
    "    img = contrast_enhancer.enhance(1.1)\n",
    "\n",
    "    # Noise reduction\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    img_array = np.array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "def load_raw_image(image_path, target_size=(384, 384)):\n",
    "    \"\"\"\n",
    "    Loads the image without preprocessing, only resizing it.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    img = img.convert(\"RGB\")\n",
    "    return np.array(img) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "ZgqcvRq_iLjs",
    "outputId": "1102432e-0e3c-41b5-bdcf-afef37a21471"
   },
   "outputs": [],
   "source": [
    "def compare_raw_vs_preprocessed_horizontal(df, num_samples=5):\n",
    "    plt.figure(figsize=(3 * num_samples, 6))  # Wider, shorter figure\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(df) - 1)\n",
    "        img_path = df.iloc[idx]['image']\n",
    "\n",
    "        raw_img = load_raw_image(img_path)\n",
    "        processed_img = preprocess_image(img_path)\n",
    "\n",
    "        # Top row: Raw image\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(raw_img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Raw Image\")\n",
    "\n",
    "        # Bottom row: Preprocessed image\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(processed_img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Preprocessed Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "compare_raw_vs_preprocessed_horizontal(df, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkXakML1jYsb"
   },
   "outputs": [],
   "source": [
    "def reduce_dataset(df, target_size=20000):\n",
    "    age_bins = df['age'].unique()\n",
    "    samples_per_group = target_size // (2 * len(age_bins))  # Half for each gender in each age group\n",
    "    reduced_df = pd.concat([\n",
    "        pd.concat([\n",
    "            df[(df['age'] == age) & (df['gender'] == gender)].sample(min(samples_per_group, len(df[(df['age'] == age) & (df['gender'] == gender)])), random_state=42)\n",
    "            for gender in [0, 1]\n",
    "        ]) for age in age_bins\n",
    "    ])\n",
    "    return reduced_df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "# Apply dataset reduction\n",
    "df = reduce_dataset(df, target_size=20000)  # Change to desired size\n",
    "\n",
    "gender_dic = {0: 'Male', 1: 'Female'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Yz6CCz7kjkiZ",
    "outputId": "c67c5fdb-2df3-436e-8236-1ad5b04099fc"
   },
   "outputs": [],
   "source": [
    "# Visualize the reduced dataset\n",
    "def visualize_dataset(df):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['age'].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Age Distribution in Reduced Dataset')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df['gender'].value_counts().plot(kind='bar', color=['blue', 'pink'], edgecolor='black')\n",
    "    plt.xticks(ticks=[0, 1], labels=['Male', 'Female'], rotation=0)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Gender Distribution in Reduced Dataset')\n",
    "    plt.show()\n",
    "\n",
    "visualize_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCZKVoYpjokf"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "598525c236a8411ebfafc51a2b3ad170",
      "cad76f3c874542bd93a2f493fd519540",
      "f0b40d1efb854d899495397044007547",
      "0389a84bd8224fa2b6f40a9194f9f3d1",
      "455c3df0b83d42f38618a5f89ac3df37",
      "34181b31da844ca1bc6552607ae52a12",
      "32c8077e62f34b5696405bee8b70628e",
      "811578ff9a0d4a1a9beaf85ea103dff2",
      "b1d399635d6c4b75939f6397a8e1f29e",
      "259346649f674de7b61d1480c8a61f19",
      "eb19a2b1b1774a21a753d6ea8c9b4622",
      "f0eda370d73a465f8cef3595e6bbeac3",
      "c5f3cf10f1344104bea6b2f62bb29064",
      "c894ef6fa19a4084bebee83587ee7ca3",
      "91fb09c9f6b8401485d30972b138c899",
      "a524ad97ccbb4ae3951901735b1a580d",
      "779c824a0220448691634b852ff52c53",
      "0a98328eaf8641e183fa86494fb99390",
      "3c8d28e165a5433fa7520d102f21cd08",
      "a64713d296d64a1d9afca69b047476e6",
      "5743927498574ee0bc8aeac7880b813f",
      "e40e86c28aa04df6b2d55242f6f564f0",
      "d190b42130494facb364b856aa89870c",
      "7b6ece9fddd94710ad3f0653f261e5c5",
      "2aca34ef5ee74a00b03eee3d69dd844c",
      "ae17127b49ca4063b86d079d9f7edc78",
      "c6bf90eb1a7448c9a5767530d9c4c7e9",
      "a659b9e2ae66475085e01fb29489530b",
      "184a46d228bd496b8d5bf0d66ccaae61",
      "0196686c0fbe4c7a80cbaf23e5a7edb5",
      "36b6cce1ff784136a42bd57241ed774f",
      "599f4de924884754843b21807f947b3d",
      "4b1467c3e9f04ab7af9d298783b619cb",
      "70a1a47799494591baa749d9cecb369e",
      "3df05937dfaf43db91c731bde53e3dc3",
      "82d17e2fa746404e912ad10fc8f94e25",
      "7c3e696083d34751b1cad6e2b51ce834",
      "d23c441fd20c44fba3f65deb016d98d2",
      "a5b0ccdba5b7460e85c2be7ad28c7fc6",
      "b6f25485d00c423d95822b6365339419",
      "c36a165d2b55472f88c9b2f40b433dd9",
      "8809f44644f34af59d3e50a7c55a5f28",
      "de713489ac6f48129644e6951b36626c",
      "70c451d9ec054fbc9cf428325c3b4345",
      "37b5b8d63c77467c8747e878af7b59f6",
      "35250bfcc0884b9a874c37eb2b4c1ec4",
      "582cb7b87ba8493ca0f4347ed45cdac3",
      "019f7072fad04d3fa5ec322629c461d8",
      "40a6720c16c941189e9e484b86da2923",
      "13474516f812476bbe7863701518d404",
      "4208d59b91134571a041187a4bce529f",
      "d2e31d30b9424a038fdd2175291b5d7e",
      "fcb14c9821c542658ce426f526172e7e",
      "a3667e5922ad4cf7ac98825c88d60e91",
      "afed8759049648aab067e33a7ff2fe84",
      "9eba9ffced60458ba93aa01cdc64db66",
      "d3955e884f194cde9f11eb7d09197989",
      "be8fb4ece05f48078ddcc86044072a55",
      "93559a761a424e58bdd058d50b9d3105",
      "314f3d52cb65467686af0fdd3b4b431c",
      "0cdf696b7dea4cf6bea44b004f480f21",
      "4ab551352920410086b3bfd0dfabb4b8",
      "c86ce2014ded4a4792dc1da9973fc6d6",
      "24c558daa4644e35a0bb0d7a84131347",
      "4210bf1e82dc47eda34fb6feab35a179",
      "ad43a368da914a7fbbc2e733cb9de0da",
      "ce703121d03542dc9446ffcd950da7cf",
      "30eb7275cc944d62b43fb859f81763ec",
      "9c9391e42c7e4672bd0c712c8350713c",
      "4bf897d04fd3421e84615e57ac50d31b",
      "1bdb2652f7124349967d5c624c66fe5e",
      "2c7145dc76264bd8abdcc1b6cad6e682",
      "4896b6f1347a49dcacf49d5301ae3e6c",
      "d13e9b909935492b9f93ca05c42c5ef9",
      "1ef3168f899a489b9f5e9711004507be",
      "5e9802b938344c028e0a506018e889be",
      "5a1c74a92e6b4efebb5b61b124dcbfc7",
      "2fccee17c13d471cb5a90a2d3bd15c08",
      "4c3005e8fd6f405bac00159e77c0c335",
      "fcb9b03d72fc46bc84d3d4e74ee31386",
      "5d6ae2cdfa564f59a1f9ed6ddb510249",
      "0e59e4c947e14f819d1c9ebc226ba6e3",
      "381015f2ef4b49098449444db26eba07",
      "8ad4e892309e47c7a0fe169052bf9984",
      "4d0778df5bcd46a0b6c52f273146f276",
      "a879fe79499b42a49462a2a5df247a17",
      "5b8966df06214b99b712bce756d7593a",
      "5d40c36670ba4ff1bdfc944bb83c7b73"
     ]
    },
    "id": "oP-3o-Pxo1sD",
    "outputId": "bd681e56-0706-401d-b4d7-766fc1d3efee"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "def extract_features(df, preprocess=False):\n",
    "    features, age_labels, gender_labels = [], [], []\n",
    "\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        img_path = df.iloc[idx]['image']\n",
    "\n",
    "        try:\n",
    "            # Load image with preprocessing if enabled\n",
    "            img = preprocess_image(img_path) if preprocess else load_raw_image(img_path)\n",
    "\n",
    "            # Convert image to PIL format & prepare input for CLIP\n",
    "            inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Move tensors to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()  # Move to CPU before NumPy conversion\n",
    "\n",
    "            # Append extracted features & labels\n",
    "            features.append(vec)\n",
    "            age_labels.append(df.iloc[idx]['age'])\n",
    "            gender_labels.append(df.iloc[idx]['gender'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {img_path}: {e}\")\n",
    "            continue  # Skip problematic images\n",
    "\n",
    "    return np.array(features), np.array(age_labels), np.array(gender_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiUUTm0wjtgD",
    "outputId": "ee964ebe-4a56-45c8-996e-d0b5100ccbe5"
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X_train_raw, y_train_age_raw, y_train_gender_raw = extract_features(train_df, preprocess=False)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save extracted features\n",
    "joblib.dump((X_train_raw, y_train_age_raw, y_train_gender_raw), \"/content/drive/MyDrive/Age Prediction/Features/X_train_raw.pkl\")\n",
    "\n",
    "print(\"Features saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dWEWAXjkAp1",
    "outputId": "d64af9ad-44d6-4621-bc0e-63eddfc7e562"
   },
   "outputs": [],
   "source": [
    "X_test_raw, y_test_age_raw, y_test_gender_raw = extract_features(test_df, preprocess=False)\n",
    "\n",
    "joblib.dump((X_test_raw, y_test_age_raw, y_test_gender_raw), \"/content/drive/MyDrive/Age Prediction/Features/X_test_raw.pkl\")\n",
    "\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMad49XElGMI",
    "outputId": "283f8f2a-4869-43e6-9e94-4b3477d694e9"
   },
   "outputs": [],
   "source": [
    "X_train_pre, y_train_age_pre, y_train_gender_pre = extract_features(train_df, preprocess=True)\n",
    "\n",
    "# Save preprocessed features\n",
    "joblib.dump((X_train_pre, y_train_age_pre, y_train_gender_pre), \"/content/drive/MyDrive/Age Prediction/Features/X_train_pre.pkl\")\n",
    "\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g1dRtI-lRh_",
    "outputId": "74730975-213c-4a3b-ad05-f1bb97a15c93"
   },
   "outputs": [],
   "source": [
    "X_test_pre, y_test_age_pre, y_test_gender_pre = extract_features(test_df, preprocess=True)\n",
    "\n",
    "joblib.dump((X_test_pre, y_test_age_pre, y_test_gender_pre), \"/content/drive/MyDrive/Age Prediction/Features/X_test_pre.pkl\")\n",
    "\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZ2HaLPiKOwO",
    "outputId": "3184f0c1-20f4-45e3-8c3f-929f042c19cd"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#Load Raw Features\n",
    "X_train_raw, y_train_age_raw, y_train_gender_raw = joblib.load(\"/content/drive/MyDrive/Age Prediction/Features/X_train_raw.pkl\")\n",
    "X_test_raw, y_test_age_raw, y_test_gender_raw = joblib.load(\"/content/drive/MyDrive/Age Prediction/Features/X_test_raw.pkl\")\n",
    "\n",
    "# Load preprocessed features\n",
    "X_train_pre, y_train_age_pre, y_train_gender_pre = joblib.load(\"/content/drive/MyDrive/Age Prediction/Features/X_train_pre.pkl\")\n",
    "X_test_pre, y_test_age_pre, y_test_gender_pre = joblib.load(\"/content/drive/MyDrive/Age Prediction/Features/X_test_pre.pkl\")\n",
    "\n",
    "print(\"Features loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bkSHIk1xRAPh",
    "outputId": "99be1bcc-5a3f-4524-f014-e970be0b2dd4"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Function to preprocess images for CLIP\n",
    "def preprocess_images(images):\n",
    "    inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    return inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "# Wrapper function that handles conversion between numpy and torch\n",
    "def model_predict(images_numpy):\n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    images_tensor = torch.tensor(images_numpy, device=device)\n",
    "\n",
    "    # Get features from CLIP\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(images_tensor)\n",
    "\n",
    "    # Return as numpy for SHAP\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Load sample images - replace with your own image paths\n",
    "image_paths = [\n",
    "    \"/root/.cache/kagglehub/datasets/jangedoo/utkface-new/versions/1/UTKFace/1_0_0_20161219154556757.jpg.chip.jpg\",\n",
    "    \"/root/.cache/kagglehub/datasets/jangedoo/utkface-new/versions/1/UTKFace/45_0_0_20170117135648206.jpg.chip.jpg\",\n",
    "    \"/root/.cache/kagglehub/datasets/jangedoo/utkface-new/versions/1/UTKFace/105_0_0_20170112213001988.jpg.chip.jpg\"\n",
    "]\n",
    "\n",
    "# Load images\n",
    "images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "\n",
    "# Preprocess the images for CLIP\n",
    "preprocessed_images = preprocess_images(images)\n",
    "\n",
    "# Try a different approach with GradientExplainer - this works better with deep models like CLIP\n",
    "background = preprocessed_images[:1]  # Use first image as background\n",
    "\n",
    "# Create the wrapper\n",
    "class CLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get a consistent 1D output by averaging the features\n",
    "        features = self.clip_model.get_image_features(x)\n",
    "        return features.mean(dim=1, keepdim=True)  # Return shape (batch_size, 1)\n",
    "\n",
    "# Create gradient explainer wrapper\n",
    "clip_wrapper = CLIPWrapper(clip_model).to(device)\n",
    "clip_wrapper.eval()\n",
    "\n",
    "# Create the explainer\n",
    "gradient_explainer = shap.GradientExplainer(clip_wrapper, background)\n",
    "\n",
    "# Generate gradient-based SHAP values\n",
    "gradient_shap_values = gradient_explainer.shap_values(preprocessed_images)\n",
    "\n",
    "# Visualize the results\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    base_name = img_path.split('/')[-1].split('.')[0]\n",
    "    img_age = base_name.split('_')[0]  # Extract age from filename\n",
    "\n",
    "    # Convert SHAP values to numpy for visualization\n",
    "    # FIX: gradient_shap_values already contains numpy arrays, no need for cpu() or to(device)\n",
    "    shap_numpy = gradient_shap_values[0][i]\n",
    "\n",
    "    # Get the shape for reshaping correctly\n",
    "    input_shape = preprocessed_images[i].cpu().numpy().shape\n",
    "\n",
    "    # Print more debug info\n",
    "    print(f\"SHAP values shape: {shap_numpy.shape}, input shape: {input_shape}\")\n",
    "\n",
    "    # Based on your output, we know SHAP values are (224, 224, 1)\n",
    "    if len(shap_numpy.shape) == 3 and shap_numpy.shape[2] == 1:\n",
    "        # We already have a spatial map, just squeeze the last dimension\n",
    "        shap_img = shap_numpy[:,:,0]  # Remove singleton dimension\n",
    "        print(f\"Using spatial SHAP map directly\")\n",
    "    elif shap_numpy.size != np.prod(input_shape):\n",
    "        # If shapes don't match, we need to handle differently\n",
    "        shap_img = shap_numpy  # Use as is without reshaping\n",
    "        print(f\"Shapes don't match, using raw SHAP values\")\n",
    "    else:\n",
    "        # Reshape to image dimensions\n",
    "        shap_img = shap_numpy.reshape(input_shape)\n",
    "        print(f\"Reshaped SHAP values to match input\")\n",
    "\n",
    "    # Take absolute values and normalize for better visualization\n",
    "    abs_shap = np.abs(shap_img)\n",
    "    max_val = abs_shap.max()\n",
    "    if max_val > 0:\n",
    "        abs_shap = abs_shap / max_val\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Original Image (Age: {img_age})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # SHAP values\n",
    "    plt.subplot(1, 3, 2)\n",
    "\n",
    "    # Handle different shapes for visualization\n",
    "    if len(shap_img.shape) == 2:\n",
    "        # Already 2D - perfect for visualization\n",
    "        shap_sum = np.abs(shap_img)\n",
    "    elif len(shap_img.shape) == 3 and shap_img.shape[2] == 1:\n",
    "        # If we have a 3D tensor with a singleton last dimension\n",
    "        shap_sum = np.abs(shap_img[:,:,0])\n",
    "    elif len(shap_img.shape) == 3:\n",
    "        # If we have 3D tensor (e.g., channels, height, width)\n",
    "        shap_sum = np.abs(shap_img).sum(axis=0)\n",
    "    else:\n",
    "        # If we have a flattened or different shape, reshape for visualization\n",
    "        # For CLIP feature visualizations, we can just show the raw values\n",
    "        # or reshape to a square for visualization\n",
    "        size = int(np.sqrt(abs_shap.size))\n",
    "        shap_sum = abs_shap.reshape(size, size)\n",
    "\n",
    "    # Normalize for better visibility\n",
    "    max_val = shap_sum.max()\n",
    "    if max_val > 0:\n",
    "        shap_sum = shap_sum / max_val\n",
    "\n",
    "    plt.imshow(shap_sum, cmap='hot')\n",
    "    plt.title(f\"SHAP Magnitude (Age: {img_age})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    img_array = np.array(images[i])\n",
    "\n",
    "    # Normalize image to [0, 1]\n",
    "    img_norm = img_array / 255.0\n",
    "\n",
    "    # Resize SHAP values to match original image regardless of original shape\n",
    "    from skimage.transform import resize\n",
    "\n",
    "    # Make sure shap_sum is 2D for resizing\n",
    "    if len(shap_sum.shape) != 2:\n",
    "        print(f\"Warning: Expected 2D SHAP values for overlay, got {shap_sum.shape}. Adjusting...\")\n",
    "        # If not 2D, reshape or sum as needed\n",
    "        if len(shap_sum.shape) > 2:\n",
    "            shap_sum = shap_sum.sum(axis=tuple(range(len(shap_sum.shape) - 2)))\n",
    "\n",
    "    # Now resize to match the original image\n",
    "    shap_resized = resize(shap_sum, (img_array.shape[0], img_array.shape[1]),\n",
    "                          order=1, mode='constant', anti_aliasing=True)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    cmap = plt.cm.hot\n",
    "    shap_heatmap = cmap(shap_resized)[:, :, :3]\n",
    "\n",
    "    # Combine with alpha blending\n",
    "    alpha = 0.7\n",
    "    overlay = alpha * shap_heatmap + (1 - alpha) * img_norm\n",
    "\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"SHAP Overlay (Age: {img_age})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"age_{img_age}_clip_explanation.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Create a comparison visualization across different ages\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get original images in a row\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    plt.subplot(2, len(image_paths), i+1)\n",
    "    plt.imshow(images[i])\n",
    "    img_age = img_path.split('/')[-1].split('.')[0].split('_')[0]\n",
    "    plt.title(f\"Age: {img_age}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get SHAP visualizations in a row\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    plt.subplot(2, len(image_paths), len(image_paths)+i+1)\n",
    "\n",
    "    # Convert SHAP values to numpy for visualization\n",
    "    shap_numpy = gradient_shap_values[0][i]\n",
    "    print(f\"Comparison visualization - SHAP shape: {shap_numpy.shape}\")\n",
    "\n",
    "    # Handle based on the actual shape we're seeing (224, 224, 1)\n",
    "    if len(shap_numpy.shape) == 3 and shap_numpy.shape[2] == 1:\n",
    "        # We have a single-channel heatmap already\n",
    "        abs_shap = np.abs(shap_numpy[:,:,0])  # Remove the singleton dimension\n",
    "    elif shap_numpy.size != np.prod(preprocessed_images[i].cpu().numpy().shape):\n",
    "        # If we can't reshape to original dimensions, create a square visualization\n",
    "        size = int(np.sqrt(shap_numpy.size))\n",
    "        abs_shap = np.abs(shap_numpy).reshape(size, size)\n",
    "    else:\n",
    "        # Reshape to image dimensions\n",
    "        shap_img = shap_numpy.reshape(preprocessed_images[i].cpu().numpy().shape)\n",
    "        # Take absolute values and sum across channels\n",
    "        abs_shap = np.abs(shap_img)\n",
    "        abs_shap = abs_shap.sum(axis=0)\n",
    "\n",
    "    # Normalize for better visibility\n",
    "    max_val = abs_shap.max()\n",
    "    if max_val > 0:\n",
    "        norm_shap = abs_shap / max_val\n",
    "    else:\n",
    "        norm_shap = abs_shap\n",
    "\n",
    "    plt.imshow(norm_shap, cmap='hot')\n",
    "    img_age = img_path.split('/')[-1].split('.')[0].split('_')[0]\n",
    "    plt.title(f\"SHAP (Age: {img_age})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"CLIP Feature Importance Across Different Ages\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig(\"age_comparison_clip_summary.png\")\n",
    "\n",
    "print(\"SHAP analysis completed. Images saved with age information.\")\n",
    "\n",
    "# Optional - Feature distribution analysis\n",
    "print(\"\\nAnalyzing feature distributions across ages...\")\n",
    "\n",
    "# Extract features directly for analysis\n",
    "with torch.no_grad():\n",
    "    features = clip_model.get_image_features(preprocessed_images)\n",
    "    features_np = features.cpu().numpy()\n",
    "\n",
    "# Create feature distribution visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    img_age = img_path.split('/')[-1].split('.')[0].split('_')[0]\n",
    "    plt.plot(features_np[i][:50], label=f\"Age {img_age}\")  # Plot first 50 features\n",
    "\n",
    "plt.title(\"CLIP Feature Distribution Across Ages\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Feature Activation\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"age_feature_distribution.png\")\n",
    "\n",
    "print(\"Feature distribution analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYwq3rfpSvLI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load YuNet Model\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "# Function to apply Grad-CAM++\n",
    "def apply_gradcam_plusplus(image_path):\n",
    "    # Read Image\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width = img.shape[:2]\n",
    "    yunet.setInputSize((width, height))\n",
    "\n",
    "    # Perform Face Detection\n",
    "    _, faces = yunet.detect(img)\n",
    "\n",
    "    if faces is None:\n",
    "        print(\"❌ No face detected!\")\n",
    "        return\n",
    "\n",
    "    # Select the first detected face\n",
    "    x, y, w, h = map(int, faces[0][:4])\n",
    "    face_crop = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Convert to Grayscale (Simulating Activation Map for YuNet)\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Simulating Feature Importance (YuNet doesn’t provide direct feature maps)\n",
    "    heatmap = cv2.applyColorMap(gray_face, cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.resize(heatmap, (w, h))\n",
    "\n",
    "    # Overlay Heatmap\n",
    "    overlayed = cv2.addWeighted(face_crop, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    # Place back in original image\n",
    "    img[y:y+h, x:x+w] = overlayed\n",
    "\n",
    "    # Show Image with Grad-CAM++ Visualization\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Grad-CAM++ for YuNet Face Detection\")\n",
    "    plt.show()\n",
    "\n",
    "# Run Grad-CAM++ for YuNet\n",
    "apply_gradcam_plusplus(\"face_image.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvQt4yxWskv_"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_features(features, labels, title):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        plt.scatter(reduced_features[labels == label, 0], reduced_features[labels == label, 1], label=label, alpha=0.6)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8nSUk3ss0Nd",
    "outputId": "e3ca71c3-854b-4346-c4d9-c1241bc4dfc1"
   },
   "outputs": [],
   "source": [
    "visualize_features(X_train_raw, y_train_age_raw, \"t-SNE Visualization of Age Features\")\n",
    "visualize_features(X_train_raw, y_train_gender_raw, \"t-SNE Visualization of Gender Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMbtBVwSs4yT"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_features(features, labels, title):\n",
    "    tsne = TSNE(n_components=3, perplexity=30, random_state=42)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        idxs = labels == label\n",
    "        ax.scatter(reduced_features[idxs, 0], reduced_features[idxs, 1], reduced_features[idxs, 2], label=label, alpha=0.6)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_zlabel(\"t-SNE Component 3\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-QET99Z8s9cK",
    "outputId": "a6cdee86-e7c7-4c9b-9280-32cf1196102b"
   },
   "outputs": [],
   "source": [
    "visualize_features(X_train_raw, y_train_age_raw, \"3D t-SNE Visualization of Age Features\")\n",
    "visualize_features(X_train_raw, y_train_gender_raw, \"3D t-SNE Visualization of Gender Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iTpNoy1I5lot",
    "outputId": "901a1248-dfd3-4c03-d23b-659afb59d7ee"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def visualize_features(features, labels, title):\n",
    "    # Perform t-SNE dimensionality reduction to 3 components\n",
    "    tsne = TSNE(n_components=3, perplexity=30, random_state=42)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "\n",
    "    # Create a 3D scatter plot using Plotly Express\n",
    "    fig = px.scatter_3d(\n",
    "        x=reduced_features[:, 0],\n",
    "        y=reduced_features[:, 1],\n",
    "        z=reduced_features[:, 2],\n",
    "        color=labels.astype(str),  # Convert labels to string for better color mapping\n",
    "        labels={'x': 't-SNE Component 1', 'y': 't-SNE Component 2', 'z': 't-SNE Component 3'},\n",
    "        title=title,\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        scene=dict(\n",
    "            xaxis_title=\"t-SNE Component 1\",\n",
    "            yaxis_title=\"t-SNE Component 2\",\n",
    "            zaxis_title=\"t-SNE Component 3\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()\n",
    "    print(f\"3D t-SNE for {title}: Rotate to explore clusters!\")\n",
    "\n",
    "# Visualize age features\n",
    "visualize_features(X_train_raw, y_train_age_raw, \"3D t-SNE Visualization of Age Features\")\n",
    "\n",
    "# Visualize gender features\n",
    "visualize_features(X_train_raw, y_train_gender_raw, \"3D t-SNE Visualization of Gender Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmpZRi2C0k62"
   },
   "outputs": [],
   "source": [
    "def visualize_training_performance(history, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.loss_curve_, label='Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def train_mlp(X_train, y_train, X_test, y_test, task=\"age\", model_filename=\"mlp_model.pkl\"):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(1024, 512, 256, 128),  # Deeper network\n",
    "        activation='relu',                   # Can change to 'tanh'\n",
    "        solver='adam',                        # 'adam' is good for large datasets\n",
    "        learning_rate_init=0.001,# Adjusts learning rate dynamically\n",
    "        alpha=0.00001,                        # L2 regularization (not learning rate)\n",
    "        max_iter=500                          # More training iterations\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Performance Metrics\n",
    "    print(f\"Accuracy for {task} prediction:\", accuracy_score(y_test, y_pred))\n",
    "    print(f\"Classification Report ({task}):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Visualization Functions (Make sure these exist)\n",
    "    visualize_confusion_matrix(y_test, y_pred, f\"Confusion Matrix - {task} Prediction\")\n",
    "    visualize_training_performance(model, f\"Training Performance - {task} Prediction\")\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNRg8b-qI_Yx"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def train_naive_bayes(X_train, y_train, X_test, y_test, task=\"age\", model_filename=\"naive_bayes.pkl\"):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy for {task} prediction:\", accuracy_score(y_test, y_pred))\n",
    "    print(f\"Classification Report ({task}):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    visualize_confusion_matrix(y_test, y_pred, f\"Confusion Matrix - {task} Prediction\")\n",
    "\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "id": "1q0dQ-N3JDm9",
    "outputId": "2b38cddb-663a-469a-fe66-fe703b69f9bc"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nTraining Age Prediction Model on Raw Images:\")\n",
    "naive_age_raw = train_naive_bayes(X_train_raw, y_train_age_raw, X_test_raw, y_test_age_raw, task=\"age\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/naive_age_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyj0NYCrJYGz"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, task=\"age\", model_filename=\"random_forest.pkl\"):\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy for {task} prediction:\", accuracy_score(y_test, y_pred))\n",
    "    print(f\"Classification Report ({task}):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    visualize_confusion_matrix(y_test, y_pred, f\"Confusion Matrix - {task} Prediction\")\n",
    "\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "id": "irfCgzaPJbUq",
    "outputId": "5f363135-0873-4c0a-bd26-fc0538577c46"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nTraining Age Prediction Model on Raw Images:\")\n",
    "rf_age_raw = train_random_forest(X_train_raw, y_train_age_raw, X_test_raw, y_test_age_raw, task=\"age\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/rf_age_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpV4xXFKJ3pF"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def train_adaboost(X_train, y_train, X_test, y_test, task=\"age\", model_filename=\"adaboost.pkl\"):\n",
    "    model = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy for {task} prediction:\", accuracy_score(y_test, y_pred))\n",
    "    print(f\"Classification Report ({task}):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    visualize_confusion_matrix(y_test, y_pred, f\"Confusion Matrix - {task} Prediction\")\n",
    "\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "85KLPUCcCtAR",
    "outputId": "07752b5f-f549-4179-d01f-66a280a4bbbf"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nTraining Age Prediction Model on Raw Images:\")\n",
    "mlp_age_raw = train_mlp(X_train_raw, y_train_age_raw, X_test_raw, y_test_age_raw, task=\"age\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MEeH-y57z2o7",
    "outputId": "55f59eed-25f2-4936-e255-b783ec9a78cb"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Gender Prediction Model on Raw Images:\")\n",
    "mlp_gender_raw = train_mlp(X_train_raw, y_train_gender_raw, X_test_raw, y_test_gender_raw, task=\"gender\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ga0nYu-9z5a8",
    "outputId": "c6758632-b643-4051-db4f-518735d8bb44"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Age Prediction Model on Preprocessed Images:\")\n",
    "mlp_age_pre = train_mlp(X_train_pre, y_train_age_pre, X_test_pre, y_test_age_pre, task=\"age\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_pre1_adv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W06K8yCmz8i6",
    "outputId": "42450644-3daf-49d6-84d5-7e9de542cf3f"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Gender Prediction Model on Preprocessed Images:\")\n",
    "mlp_gender_pre = train_mlp(X_train_pre, y_train_gender_pre, X_test_pre, y_test_gender_pre, task=\"gender\", model_filename=\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_pre1_adv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oQc7S7Z7cicf",
    "outputId": "45e3c3f1-9ec0-47b2-d37c-e176d8aa5c91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load pre-trained MLP models for age and gender prediction\n",
    "age_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")\n",
    "gender_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")\n",
    "\n",
    "# Initialize YuNet for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 10:\n",
    "        return \"0-10\"\n",
    "    elif age <= 20:\n",
    "        return \"11-20\"\n",
    "    elif age <= 30:\n",
    "        return \"21-30\"\n",
    "    elif age <= 40:\n",
    "        return \"31-40\"\n",
    "    elif age <= 50:\n",
    "        return \"41-50\"\n",
    "    else:\n",
    "        return \"50+\"\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Apply DIP techniques: contrast enhancement, noise reduction.\"\"\"\n",
    "    img = Image.fromarray(img).convert(\"RGB\")\n",
    "    img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "    return np.array(img) / 255.0\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract features using CLIP model.\"\"\"\n",
    "    inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "def predict_age_gender(features):\n",
    "    \"\"\"Predict age and gender using MLP models.\"\"\"\n",
    "    age_label = age_model.predict([features])[0]  # Get predicted age category\n",
    "    gender_pred = \"Male\" if gender_model.predict([features])[0] == 0 else \"Female\"\n",
    "    return age_label, gender_pred\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🔹 Processing Image: {filename}\")\n",
    "\n",
    "        age_groups = defaultdict(int)  # Reset count for each image\n",
    "\n",
    "        yunet.setInputSize((frame.shape[1], frame.shape[0]))\n",
    "        _, faces = yunet.detect(frame)  # Detect faces\n",
    "        frame_raw = frame.copy()\n",
    "        frame_dip = frame.copy()\n",
    "\n",
    "        # Raw processing (without DIP)\n",
    "        frame_raw_resized = cv2.resize(frame_raw, (128, 128)) / 255.0\n",
    "        features_raw = extract_features(frame_raw_resized)\n",
    "        age_raw, gender_raw = predict_age_gender(features_raw)\n",
    "        cv2.putText(frame_raw, f\"Raw: {age_raw}, {gender_raw}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Count in age groups\n",
    "        age_groups[age_raw] += 1\n",
    "\n",
    "        if faces is not None:\n",
    "            for face in faces:\n",
    "                x, y, w, h = map(int, face[:4])\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_dip = preprocess_image(face_crop)\n",
    "                features_dip = extract_features(face_dip)\n",
    "                age_dip, gender_dip = predict_age_gender(features_dip)\n",
    "                cv2.rectangle(frame_dip, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame_dip, f\"DIP: {age_dip}, {gender_dip}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "                # Count in age groups (DIP method)\n",
    "                age_groups[age_dip] += 1\n",
    "\n",
    "        combined_frame = np.hstack((frame_raw, frame_dip))\n",
    "        cv2_imshow(combined_frame)\n",
    "\n",
    "        # Print Summary **for the current image**\n",
    "        print(\"\\n📊 **Summary for Image:**\")\n",
    "        for group, count in age_groups.items():\n",
    "            print(f\"{group} years: {count} people\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/drive/MyDrive/Age Prediction/Real Time Test Data\"  # Folder path\n",
    "    process_images_in_folder(folder_path)  # Run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZQ3gUcW1ju-p",
    "outputId": "8c82df1c-3de2-4d28-db70-3f71224013b3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load pre-trained MLP models for age and gender prediction\n",
    "age_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")\n",
    "gender_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")\n",
    "\n",
    "# Initialize YuNet for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 10:\n",
    "        return \"0-10\"\n",
    "    elif age <= 20:\n",
    "        return \"11-20\"\n",
    "    elif age <= 30:\n",
    "        return \"21-30\"\n",
    "    elif age <= 40:\n",
    "        return \"31-40\"\n",
    "    elif age <= 50:\n",
    "        return \"41-50\"\n",
    "    else:\n",
    "        return \"50+\"\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Apply DIP techniques: contrast enhancement, noise reduction.\"\"\"\n",
    "    img = Image.fromarray(img).convert(\"RGB\")\n",
    "    img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "    return np.array(img) / 255.0\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract features using CLIP model.\"\"\"\n",
    "    inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "def predict_age_gender(features):\n",
    "    \"\"\"Predict age and gender using MLP models.\"\"\"\n",
    "    age_label = age_model.predict([features])[0]  # Get predicted age category\n",
    "    gender_pred = \"Male\" if gender_model.predict([features])[0] == 0 else \"Female\"\n",
    "    return age_label, gender_pred\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🔹 Processing Image: {filename}\")\n",
    "\n",
    "        age_groups = defaultdict(int)  # Reset count for each image\n",
    "\n",
    "        yunet.setInputSize((frame.shape[1], frame.shape[0]))\n",
    "        _, faces = yunet.detect(frame)  # Detect faces\n",
    "        frame_raw = frame.copy()\n",
    "        frame_dip = frame.copy()\n",
    "\n",
    "        # Raw processing (without DIP)\n",
    "        frame_raw_resized = cv2.resize(frame_raw, (128, 128)) / 255.0\n",
    "        features_raw = extract_features(frame_raw_resized)\n",
    "        age_raw, gender_raw = predict_age_gender(features_raw)\n",
    "        cv2.putText(frame_raw, f\"Raw: {age_raw}, {gender_raw}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Count in age groups\n",
    "        age_groups[age_raw] += 1\n",
    "\n",
    "        if faces is not None:\n",
    "            for face in faces:\n",
    "                x, y, w, h = map(int, face[:4])\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_dip = preprocess_image(face_crop)\n",
    "                features_dip = extract_features(face_dip)\n",
    "                age_dip, gender_dip = predict_age_gender(features_dip)\n",
    "                cv2.rectangle(frame_dip, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame_dip, f\"DIP: {age_dip}, {gender_dip}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "                # Count in age groups (DIP method)\n",
    "                age_groups[age_dip] += 1\n",
    "\n",
    "        combined_frame = np.hstack((frame_raw, frame_dip))\n",
    "        cv2_imshow(combined_frame)\n",
    "\n",
    "        # Print Summary **for the current image**\n",
    "        print(\"\\n📊 **Summary for Image:**\")\n",
    "        for group, count in age_groups.items():\n",
    "            print(f\"{group} years: {count} people\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/drive/MyDrive/Age Prediction/Real Time Test Data/Test 1\"  # Folder path\n",
    "    process_images_in_folder(folder_path)  # Run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cPj-nPqyCLGd",
    "outputId": "0bf6362d-7a31-41f1-9491-6093e7951b74"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load pre-trained MLP models for age and gender prediction\n",
    "age_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")\n",
    "gender_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")\n",
    "\n",
    "# Initialize YuNet for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 10:\n",
    "        return \"0-10\"\n",
    "    elif age <= 20:\n",
    "        return \"11-20\"\n",
    "    elif age <= 30:\n",
    "        return \"21-30\"\n",
    "    elif age <= 40:\n",
    "        return \"31-40\"\n",
    "    elif age <= 50:\n",
    "        return \"41-50\"\n",
    "    else:\n",
    "        return \"50+\"\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Apply DIP techniques: contrast enhancement, noise reduction.\"\"\"\n",
    "    img = Image.fromarray(img).convert(\"RGB\")\n",
    "    img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "    return np.array(img) / 255.0\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract features using CLIP model.\"\"\"\n",
    "    inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "def predict_age_gender(features):\n",
    "    \"\"\"Predict age and gender using MLP models.\"\"\"\n",
    "    age_label = age_model.predict([features])[0]  # Get predicted age category\n",
    "    gender_pred = \"Male\" if gender_model.predict([features])[0] == 0 else \"Female\"\n",
    "    return age_label, gender_pred\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🔹 Processing Image: {filename}\")\n",
    "\n",
    "        age_groups = defaultdict(int)  # Reset count for each image\n",
    "\n",
    "        yunet.setInputSize((frame.shape[1], frame.shape[0]))\n",
    "        _, faces = yunet.detect(frame)  # Detect faces\n",
    "        frame_raw = frame.copy()\n",
    "        frame_dip = frame.copy()\n",
    "\n",
    "        # Raw processing (without DIP)\n",
    "        frame_raw_resized = cv2.resize(frame_raw, (128, 128)) / 255.0\n",
    "        features_raw = extract_features(frame_raw_resized)\n",
    "        age_raw, gender_raw = predict_age_gender(features_raw)\n",
    "        cv2.putText(frame_raw, f\"Raw: {age_raw}, {gender_raw}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Count in age groups\n",
    "        age_groups[age_raw] += 1\n",
    "\n",
    "        if faces is not None:\n",
    "            for face in faces:\n",
    "                x, y, w, h = map(int, face[:4])\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_dip = preprocess_image(face_crop)\n",
    "                features_dip = extract_features(face_dip)\n",
    "                age_dip, gender_dip = predict_age_gender(features_dip)\n",
    "                cv2.rectangle(frame_dip, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame_dip, f\"DIP: {age_dip}, {gender_dip}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "                # Count in age groups (DIP method)\n",
    "                age_groups[age_dip] += 1\n",
    "\n",
    "        combined_frame = np.hstack((frame_raw, frame_dip))\n",
    "        cv2_imshow(combined_frame)\n",
    "\n",
    "        # Print Summary **for the current image**\n",
    "        print(\"\\n📊 **Summary for Image:**\")\n",
    "        for group, count in age_groups.items():\n",
    "            print(f\"{group} years: {count} people\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/drive/MyDrive/Age Prediction/Real Time Test Data/Test 2\"  # Folder path\n",
    "    process_images_in_folder(folder_path)  # Run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sEtkd0eJkiGC",
    "outputId": "e9935531-589f-439b-8150-1d782a530c9f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load pre-trained MLP models for age and gender prediction\n",
    "age_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")\n",
    "gender_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")\n",
    "\n",
    "# Initialize YuNet for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 10:\n",
    "        return \"0-10\"\n",
    "    elif age <= 20:\n",
    "        return \"11-20\"\n",
    "    elif age <= 30:\n",
    "        return \"21-30\"\n",
    "    elif age <= 40:\n",
    "        return \"31-40\"\n",
    "    elif age <= 50:\n",
    "        return \"41-50\"\n",
    "    else:\n",
    "        return \"50+\"\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Apply DIP techniques: contrast enhancement, noise reduction.\"\"\"\n",
    "    img = Image.fromarray(img).convert(\"RGB\")\n",
    "    img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "    return np.array(img) / 255.0\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract features using CLIP model.\"\"\"\n",
    "    inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "def predict_age_gender(features):\n",
    "    \"\"\"Predict age and gender using MLP models.\"\"\"\n",
    "    age_label = age_model.predict([features])[0]  # Get predicted age category\n",
    "    gender_pred = \"Male\" if gender_model.predict([features])[0] == 0 else \"Female\"\n",
    "    return age_label, gender_pred\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🔹 Processing Image: {filename}\")\n",
    "\n",
    "        age_gender_count = defaultdict(lambda: defaultdict(int))  # Nested dictionary for age and gender counts\n",
    "\n",
    "        yunet.setInputSize((frame.shape[1], frame.shape[0]))\n",
    "        _, faces = yunet.detect(frame)  # Detect faces\n",
    "        frame_raw = frame.copy()\n",
    "        frame_dip = frame.copy()\n",
    "\n",
    "        # Raw processing (without DIP)\n",
    "        frame_raw_resized = cv2.resize(frame_raw, (128, 128)) / 255.0\n",
    "        features_raw = extract_features(frame_raw_resized)\n",
    "        age_raw, gender_raw = predict_age_gender(features_raw)\n",
    "        cv2.putText(frame_raw, f\"Raw: {age_raw}, {gender_raw}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Count in age-gender groups\n",
    "        age_gender_count[age_raw][gender_raw] += 1\n",
    "\n",
    "        if faces is not None:\n",
    "            for face in faces:\n",
    "                x, y, w, h = map(int, face[:4])\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_dip = preprocess_image(face_crop)\n",
    "                features_dip = extract_features(face_dip)\n",
    "                age_dip, gender_dip = predict_age_gender(features_dip)\n",
    "                cv2.rectangle(frame_dip, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame_dip, f\"DIP: {age_dip}, {gender_dip}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "                # Count in age-gender groups (DIP method)\n",
    "                age_gender_count[age_dip][gender_dip] += 1\n",
    "\n",
    "        combined_frame = np.hstack((frame_raw, frame_dip))\n",
    "        cv2_imshow(combined_frame)\n",
    "\n",
    "        # Print Summary **for the current image**\n",
    "        print(\"\\n📊 **Summary for Image:**\")\n",
    "        for age_group, gender_dict in age_gender_count.items():\n",
    "            for gender, count in gender_dict.items():\n",
    "                print(f\"{age_group} years ({gender}): {count} people\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/drive/MyDrive/Age Prediction/Real Time Test Data\"  # Folder path\n",
    "    process_images_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x77sCVh063RG",
    "outputId": "bd72f21b-6c89-41c4-ae38-bba89f4c67e7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load pre-trained MLP models\n",
    "age_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_age_raw_adv.pkl\")\n",
    "gender_model = joblib.load(\"/content/drive/MyDrive/Age Prediction/Models/mlp_gender_raw1_adv.pkl\")\n",
    "\n",
    "# Initialize YuNet for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model='/content/drive/MyDrive/Age Prediction/Models/face_detection_yunet_2023mar.onnx',\n",
    "    config='',\n",
    "    input_size=(320, 320),\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "# Age and gender count storage\n",
    "age_distribution = {}\n",
    "gender_count = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Preprocess the image: Resize & Normalize\"\"\"\n",
    "    img = Image.fromarray(img).convert(\"RGB\")\n",
    "    img = img.resize((384, 384), Image.Resampling.LANCZOS)\n",
    "    return np.array(img) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract features using CLIP\"\"\"\n",
    "    inputs = clip_processor(images=Image.fromarray((img * 255).astype(np.uint8)), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vec = clip_model.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "    return vec\n",
    "\n",
    "def predict_age_gender(features):\n",
    "    \"\"\"Predict age and gender using MLP models\"\"\"\n",
    "    age_label = age_model.predict([features])[0]  # Age category\n",
    "    gender_pred = \"Male\" if gender_model.predict([features])[0] == 0 else \"Female\"\n",
    "    return age_label, gender_pred\n",
    "\n",
    "def process_video(video_path, output_path=\"output_video.mp4\"):\n",
    "    global age_distribution, gender_count\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        yunet.setInputSize((frame.shape[1], frame.shape[0]))\n",
    "        _, faces = yunet.detect(frame)\n",
    "\n",
    "        if faces is not None:\n",
    "            for face in faces:\n",
    "                x, y, w, h = map(int, face[:4])\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_dip = preprocess_image(face_crop)\n",
    "                features_dip = extract_features(face_dip)\n",
    "                age_dip, gender_dip = predict_age_gender(features_dip)\n",
    "\n",
    "                # Update count\n",
    "                gender_count[gender_dip] += 1\n",
    "                age_distribution[age_dip] = age_distribution.get(age_dip, 0) + 1\n",
    "\n",
    "                # Draw bounding box & overlay predictions\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{age_dip}, {gender_dip}\", (x, y-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        # Display summary at top-left corner\n",
    "        summary_text = f\"Male: {gender_count['Male']} | Female: {gender_count['Female']}\"\n",
    "        cv2.putText(frame, summary_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        age_text = \"Age Dist: \" + \", \".join([f\"{k}: {v}\" for k, v in age_distribution.items()])\n",
    "        cv2.putText(frame, age_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0:\n",
    "            print(f\"🔹 Processed {frame_count} frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"✅ Video processing complete!\")\n",
    "\n",
    "def display_video(video_path):\n",
    "    \"\"\"Display video in Google Colab\"\"\"\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f'<video width=700 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/content/drive/MyDrive/Age Prediction/Real Time Test Data/test_video1 - Made with Clipchamp_1741793443494.mp4\"\n",
    "    output_path = \"/content/drive/MyDrive/Age Prediction/processed_video.mp4\"\n",
    "\n",
    "    process_video(video_path, output_path)\n",
    "    display_video(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YjSoHic_j11"
   },
   "outputs": [],
   "source": [
    "def display_video(video_path):\n",
    "    \"\"\"Display video in Google Colab\"\"\"\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f\"\"\"<video width=700 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/content/drive/MyDrive/Age Prediction/processed_video.mp4\"\n",
    "    display_video(video_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0196686c0fbe4c7a80cbaf23e5a7edb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "019f7072fad04d3fa5ec322629c461d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3667e5922ad4cf7ac98825c88d60e91",
      "placeholder": "​",
      "style": "IPY_MODEL_afed8759049648aab067e33a7ff2fe84",
      "value": " 961k/961k [00:00&lt;00:00, 4.83MB/s]"
     }
    },
    "0389a84bd8224fa2b6f40a9194f9f3d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_259346649f674de7b61d1480c8a61f19",
      "placeholder": "​",
      "style": "IPY_MODEL_eb19a2b1b1774a21a753d6ea8c9b4622",
      "value": " 4.52k/4.52k [00:00&lt;00:00, 247kB/s]"
     }
    },
    "0a98328eaf8641e183fa86494fb99390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cdf696b7dea4cf6bea44b004f480f21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e59e4c947e14f819d1c9ebc226ba6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f906018b14b4bb3a35750abbf336be6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13474516f812476bbe7863701518d404": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "184a46d228bd496b8d5bf0d66ccaae61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bdb2652f7124349967d5c624c66fe5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ef3168f899a489b9f5e9711004507be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1fc860b218294d439b6211b5365bc1bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24c558daa4644e35a0bb0d7a84131347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "259346649f674de7b61d1480c8a61f19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2aca34ef5ee74a00b03eee3d69dd844c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0196686c0fbe4c7a80cbaf23e5a7edb5",
      "max": 316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36b6cce1ff784136a42bd57241ed774f",
      "value": 316
     }
    },
    "2c7145dc76264bd8abdcc1b6cad6e682": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fccee17c13d471cb5a90a2d3bd15c08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c3005e8fd6f405bac00159e77c0c335",
       "IPY_MODEL_fcb9b03d72fc46bc84d3d4e74ee31386",
       "IPY_MODEL_5d6ae2cdfa564f59a1f9ed6ddb510249"
      ],
      "layout": "IPY_MODEL_0e59e4c947e14f819d1c9ebc226ba6e3"
     }
    },
    "30eb7275cc944d62b43fb859f81763ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c7145dc76264bd8abdcc1b6cad6e682",
      "placeholder": "​",
      "style": "IPY_MODEL_4896b6f1347a49dcacf49d5301ae3e6c",
      "value": "tokenizer.json: 100%"
     }
    },
    "314f3d52cb65467686af0fdd3b4b431c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32c8077e62f34b5696405bee8b70628e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34181b31da844ca1bc6552607ae52a12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35250bfcc0884b9a874c37eb2b4c1ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13474516f812476bbe7863701518d404",
      "placeholder": "​",
      "style": "IPY_MODEL_4208d59b91134571a041187a4bce529f",
      "value": "vocab.json: 100%"
     }
    },
    "36b6cce1ff784136a42bd57241ed774f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37b5b8d63c77467c8747e878af7b59f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35250bfcc0884b9a874c37eb2b4c1ec4",
       "IPY_MODEL_582cb7b87ba8493ca0f4347ed45cdac3",
       "IPY_MODEL_019f7072fad04d3fa5ec322629c461d8"
      ],
      "layout": "IPY_MODEL_40a6720c16c941189e9e484b86da2923"
     }
    },
    "381015f2ef4b49098449444db26eba07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c8d28e165a5433fa7520d102f21cd08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3df05937dfaf43db91c731bde53e3dc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5b0ccdba5b7460e85c2be7ad28c7fc6",
      "placeholder": "​",
      "style": "IPY_MODEL_b6f25485d00c423d95822b6365339419",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "40a6720c16c941189e9e484b86da2923": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4208d59b91134571a041187a4bce529f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4210bf1e82dc47eda34fb6feab35a179": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "455c3df0b83d42f38618a5f89ac3df37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4896b6f1347a49dcacf49d5301ae3e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ab551352920410086b3bfd0dfabb4b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b1467c3e9f04ab7af9d298783b619cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bf897d04fd3421e84615e57ac50d31b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e9802b938344c028e0a506018e889be",
      "placeholder": "​",
      "style": "IPY_MODEL_5a1c74a92e6b4efebb5b61b124dcbfc7",
      "value": " 2.22M/2.22M [00:00&lt;00:00, 15.6MB/s]"
     }
    },
    "4c3005e8fd6f405bac00159e77c0c335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_381015f2ef4b49098449444db26eba07",
      "placeholder": "​",
      "style": "IPY_MODEL_8ad4e892309e47c7a0fe169052bf9984",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "4d0778df5bcd46a0b6c52f273146f276": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5743927498574ee0bc8aeac7880b813f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "582cb7b87ba8493ca0f4347ed45cdac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2e31d30b9424a038fdd2175291b5d7e",
      "max": 961143,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fcb14c9821c542658ce426f526172e7e",
      "value": 961143
     }
    },
    "598525c236a8411ebfafc51a2b3ad170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cad76f3c874542bd93a2f493fd519540",
       "IPY_MODEL_f0b40d1efb854d899495397044007547",
       "IPY_MODEL_0389a84bd8224fa2b6f40a9194f9f3d1"
      ],
      "layout": "IPY_MODEL_455c3df0b83d42f38618a5f89ac3df37"
     }
    },
    "599f4de924884754843b21807f947b3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a1c74a92e6b4efebb5b61b124dcbfc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b8966df06214b99b712bce756d7593a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d40c36670ba4ff1bdfc944bb83c7b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d6ae2cdfa564f59a1f9ed6ddb510249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b8966df06214b99b712bce756d7593a",
      "placeholder": "​",
      "style": "IPY_MODEL_5d40c36670ba4ff1bdfc944bb83c7b73",
      "value": " 389/389 [00:00&lt;00:00, 42.7kB/s]"
     }
    },
    "5e9802b938344c028e0a506018e889be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bf6c9cfc3914f13b8a4d453c2e3e889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70a1a47799494591baa749d9cecb369e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3df05937dfaf43db91c731bde53e3dc3",
       "IPY_MODEL_82d17e2fa746404e912ad10fc8f94e25",
       "IPY_MODEL_7c3e696083d34751b1cad6e2b51ce834"
      ],
      "layout": "IPY_MODEL_d23c441fd20c44fba3f65deb016d98d2"
     }
    },
    "70c451d9ec054fbc9cf428325c3b4345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "779c824a0220448691634b852ff52c53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b6ece9fddd94710ad3f0653f261e5c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a659b9e2ae66475085e01fb29489530b",
      "placeholder": "​",
      "style": "IPY_MODEL_184a46d228bd496b8d5bf0d66ccaae61",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "7c3e696083d34751b1cad6e2b51ce834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de713489ac6f48129644e6951b36626c",
      "placeholder": "​",
      "style": "IPY_MODEL_70c451d9ec054fbc9cf428325c3b4345",
      "value": " 905/905 [00:00&lt;00:00, 73.5kB/s]"
     }
    },
    "811578ff9a0d4a1a9beaf85ea103dff2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82d17e2fa746404e912ad10fc8f94e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c36a165d2b55472f88c9b2f40b433dd9",
      "max": 905,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8809f44644f34af59d3e50a7c55a5f28",
      "value": 905
     }
    },
    "8464c8c916d44852a80b743681f8429e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_feb7f1bc245b48e2a64520c608569ea6",
       "IPY_MODEL_a86e6ec52ed7470ba678662119e7c888",
       "IPY_MODEL_a40f1731fcea47959a7773a2d1373ea7"
      ],
      "layout": "IPY_MODEL_ccbf1d974dfd43469182c31b7df5af6f"
     }
    },
    "8809f44644f34af59d3e50a7c55a5f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ad4e892309e47c7a0fe169052bf9984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91fb09c9f6b8401485d30972b138c899": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5743927498574ee0bc8aeac7880b813f",
      "placeholder": "​",
      "style": "IPY_MODEL_e40e86c28aa04df6b2d55242f6f564f0",
      "value": " 1.71G/1.71G [00:08&lt;00:00, 182MB/s]"
     }
    },
    "93559a761a424e58bdd058d50b9d3105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4210bf1e82dc47eda34fb6feab35a179",
      "placeholder": "​",
      "style": "IPY_MODEL_ad43a368da914a7fbbc2e733cb9de0da",
      "value": " 525k/525k [00:00&lt;00:00, 3.70MB/s]"
     }
    },
    "9c9391e42c7e4672bd0c712c8350713c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d13e9b909935492b9f93ca05c42c5ef9",
      "max": 2224003,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ef3168f899a489b9f5e9711004507be",
      "value": 2224003
     }
    },
    "9eba9ffced60458ba93aa01cdc64db66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d3955e884f194cde9f11eb7d09197989",
       "IPY_MODEL_be8fb4ece05f48078ddcc86044072a55",
       "IPY_MODEL_93559a761a424e58bdd058d50b9d3105"
      ],
      "layout": "IPY_MODEL_314f3d52cb65467686af0fdd3b4b431c"
     }
    },
    "a3667e5922ad4cf7ac98825c88d60e91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40f1731fcea47959a7773a2d1373ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c940e264f7814943991da5f65eefe154",
      "placeholder": "​",
      "style": "IPY_MODEL_d4af5433a6b644a39b4879dac9913b94",
      "value": " 23708/23708 [00:00&lt;00:00, 205734.20it/s]"
     }
    },
    "a524ad97ccbb4ae3951901735b1a580d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5b0ccdba5b7460e85c2be7ad28c7fc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a64713d296d64a1d9afca69b047476e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a659b9e2ae66475085e01fb29489530b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a86e6ec52ed7470ba678662119e7c888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc860b218294d439b6211b5365bc1bf",
      "max": 23708,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bf6c9cfc3914f13b8a4d453c2e3e889",
      "value": 23708
     }
    },
    "a879fe79499b42a49462a2a5df247a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad43a368da914a7fbbc2e733cb9de0da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae17127b49ca4063b86d079d9f7edc78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_599f4de924884754843b21807f947b3d",
      "placeholder": "​",
      "style": "IPY_MODEL_4b1467c3e9f04ab7af9d298783b619cb",
      "value": " 316/316 [00:00&lt;00:00, 33.8kB/s]"
     }
    },
    "afed8759049648aab067e33a7ff2fe84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1d399635d6c4b75939f6397a8e1f29e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6f25485d00c423d95822b6365339419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be8fb4ece05f48078ddcc86044072a55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c86ce2014ded4a4792dc1da9973fc6d6",
      "max": 524619,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24c558daa4644e35a0bb0d7a84131347",
      "value": 524619
     }
    },
    "c36a165d2b55472f88c9b2f40b433dd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f3cf10f1344104bea6b2f62bb29064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_779c824a0220448691634b852ff52c53",
      "placeholder": "​",
      "style": "IPY_MODEL_0a98328eaf8641e183fa86494fb99390",
      "value": "model.safetensors: 100%"
     }
    },
    "c6bf90eb1a7448c9a5767530d9c4c7e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c86ce2014ded4a4792dc1da9973fc6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c894ef6fa19a4084bebee83587ee7ca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c8d28e165a5433fa7520d102f21cd08",
      "max": 1710540580,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a64713d296d64a1d9afca69b047476e6",
      "value": 1710540580
     }
    },
    "c940e264f7814943991da5f65eefe154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cad76f3c874542bd93a2f493fd519540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34181b31da844ca1bc6552607ae52a12",
      "placeholder": "​",
      "style": "IPY_MODEL_32c8077e62f34b5696405bee8b70628e",
      "value": "config.json: 100%"
     }
    },
    "ccbf1d974dfd43469182c31b7df5af6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce703121d03542dc9446ffcd950da7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30eb7275cc944d62b43fb859f81763ec",
       "IPY_MODEL_9c9391e42c7e4672bd0c712c8350713c",
       "IPY_MODEL_4bf897d04fd3421e84615e57ac50d31b"
      ],
      "layout": "IPY_MODEL_1bdb2652f7124349967d5c624c66fe5e"
     }
    },
    "d13e9b909935492b9f93ca05c42c5ef9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d190b42130494facb364b856aa89870c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b6ece9fddd94710ad3f0653f261e5c5",
       "IPY_MODEL_2aca34ef5ee74a00b03eee3d69dd844c",
       "IPY_MODEL_ae17127b49ca4063b86d079d9f7edc78"
      ],
      "layout": "IPY_MODEL_c6bf90eb1a7448c9a5767530d9c4c7e9"
     }
    },
    "d23c441fd20c44fba3f65deb016d98d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2e31d30b9424a038fdd2175291b5d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3955e884f194cde9f11eb7d09197989": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cdf696b7dea4cf6bea44b004f480f21",
      "placeholder": "​",
      "style": "IPY_MODEL_4ab551352920410086b3bfd0dfabb4b8",
      "value": "merges.txt: 100%"
     }
    },
    "d4af5433a6b644a39b4879dac9913b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de713489ac6f48129644e6951b36626c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e40e86c28aa04df6b2d55242f6f564f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb19a2b1b1774a21a753d6ea8c9b4622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0b40d1efb854d899495397044007547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_811578ff9a0d4a1a9beaf85ea103dff2",
      "max": 4519,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1d399635d6c4b75939f6397a8e1f29e",
      "value": 4519
     }
    },
    "f0eda370d73a465f8cef3595e6bbeac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5f3cf10f1344104bea6b2f62bb29064",
       "IPY_MODEL_c894ef6fa19a4084bebee83587ee7ca3",
       "IPY_MODEL_91fb09c9f6b8401485d30972b138c899"
      ],
      "layout": "IPY_MODEL_a524ad97ccbb4ae3951901735b1a580d"
     }
    },
    "fcb14c9821c542658ce426f526172e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fcb9b03d72fc46bc84d3d4e74ee31386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d0778df5bcd46a0b6c52f273146f276",
      "max": 389,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a879fe79499b42a49462a2a5df247a17",
      "value": 389
     }
    },
    "fe04e0727e85455c8f845c63a99625fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "feb7f1bc245b48e2a64520c608569ea6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f906018b14b4bb3a35750abbf336be6",
      "placeholder": "​",
      "style": "IPY_MODEL_fe04e0727e85455c8f845c63a99625fd",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
